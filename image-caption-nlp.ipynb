{"metadata":{"kernelspec":{"name":"python394jvsc74a57bd09beb1dd2404668c17887a4a2a624b1fc3db93f9c2fe13b9cdc9ee7ae2898e375","display_name":"Python 3.9.4 64-bit"},"language_info":{"name":"python","version":"3.9.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":[" **Model Evaluation Matrix**\n","\n","BLEU (Bilingual Evaluation Understudy)\n","\n","BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations.\n","\n","Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks.\n","\n","BLEU measures the closeness of the machine translation to human reference translation taking translation length, word choice, and word order into consideration. It is used for machine translation, abstractive text summarization, image captioning and speech recognition"],"metadata":{}},{"cell_type":"markdown","source":["**Importing Necessary Libraries**"],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\n","import keras\n","import sys, time, os, warnings\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","warnings.filterwarnings(\"ignore\")\n","print(\"python {}\".format(sys.version))\n","print(\"keras version {}\".format(keras.__version__));del keras\n","print(\"tensorflow version {}\".format(tf.__version__))\n","\n","# # configuring gpu notebook\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.per_process_gpu_memory_fraction = 0.95\n","config.gpu_options.visible_device_list = \"0\"\n","tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n","\n","def set_seed(sd=123):\n","    from numpy.random import seed\n","    from tensorflow import set_random_seed\n","    import random as rn\n","\n","    ##numpy random seed\n","    seed(sd)\n","    ## core python's random number\n","    rn.seed(sd)\n","    ## tensor flow's random number \n","    set_random_seed(sd)\n"],"metadata":{"trusted":true},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["python 3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]\nkeras version 2.5.0\ntensorflow version 2.5.0-rc3\n"]}]},{"cell_type":"markdown","source":["**Downloading and importing Dataset in Notebook**"],"metadata":{}},{"cell_type":"code","source":["## The location of the caption file\n","dir_Flickr_txt =\"C:/Users/Omshree/Desktop/NLP/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt\"\n","\n","## The location of the Flickr8K_ Photos\n","dir_Flickr_jpg =\"C:/Users/Omshree/Desktop/NLP/Flickr_Data/Flickr_Data/Images\"\n","\n","\n","jpgs = os.listdir(dir_Flickr_jpg)\n","print(\"The number of jpg files in Flicker8K: {}\".format(len(jpgs)))"],"metadata":{"trusted":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of jpg files in Flicker8K: 8091\n"]}]},{"cell_type":"markdown","source":["**Preliminary Analysis**\n","\n","Importing Caption Data\n","\n","Load the text data and save it into a panda dataframework df_txt"],"metadata":{}},{"cell_type":"code","source":["## read in the Flickr caption data\n","file = open(dir_Flickr_txt,'r')\n","text = file.read()\n","file.close()\n","\n","datatxt = []\n","for line in text.split('\\n'):\n","    col = line.split('\\t')\n","    if len(col) == 1:\n","        continue\n","    w = col[0].split(\"#\")\n","    datatxt.append(w + [col[1].lower()])\n","df_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n","\n","uni_filenames = np.unique(df_txt.filename.values)\n","print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n","print(\"The distributation of numbers of captions for each image :\")\n","Counter(Counter(df_txt.filename.values).values())"],"metadata":{"trusted":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of unique file names : 8092\nThe distributation of numbers of captions for each image :\n"]},{"output_type":"execute_result","data":{"text/plain":["Counter({5: 8092})"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["**Example Picture with Captions**"],"metadata":{}},{"cell_type":"code","source":["from keras.preprocessing.image import load_img, img_to_array\n","import matplotlib.pyplot as plt\n","\n","# Display (npic = 5) pic from dataset \n","npic = 5\n","npix = 244\n","target_size = (npix,npix,3)\n","\n","count = 1\n","fig = plt.figure(figsize=(10,20))\n","for jpgfnm in uni_filenames[:npic]:\n","    filename = dir_Flickr_jpg + '/' + jpgfnm\n","    captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n","    image_load = load_img(filename, target_size=target_size)\n","\n","    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n","    ax.imshow(image_load)\n","    count += 1\n","\n","    ax = fig.add_subplot(npic,2,count)\n","    plt.axis('off')\n","    ax.plot()\n","    ax.set_xlim(0,1)\n","    ax.set_ylim(0,len(captions))\n","    for i, caption in enumerate(captions):\n","        ax.text(0,i,caption,fontsize=20)\n","    count += 1\n","plt.show()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def df_word(df_txt):\n","    vocabulary = []\n","    for txt in df_txt.caption.values:\n","        vocabulary.extend(txt.split())\n","    print('Vocabulary Size: %d' % len(set(vocabulary)))\n","    ct = Counter(vocabulary)\n","    dfword = pd.DataFrame(list(ct.items()), columns=['word', 'count'])\n","    dfword.sort_values(by='count', ascending=False, inplace=True)\n","    dfword = dfword.reset_index()[[\"word\", \"count\"]]\n","    return(dfword)\n","dfword = df_word(df_txt)\n","dfword.head(3)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topn = 50\n","\n","def plthist(dfsub, title=\"The Top 50 Most Frequently Appearing Words\"):\n","    plt.figure(figsize=(20,3))\n","    plt.bar(dfsub.index,dfsub[\"count\"])\n","    plt.yticks(fontsize=20)\n","    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n","    plt.title(title,fontsize=20)\n","    plt.show()\n","\n","plthist(dfword.iloc[:topn,:], title=\"The Top 50 Most Frequently Appearing Words\")\n","plthist(dfword.iloc[-topn:,:], title=\"The least 50 Most Frequently Appearing Words\")"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import string\n","text_original = \" hi. I am Omshree !, my roll no is 08\"\n","def remove_punctuation(text_original):\n","    text_no_punctuation = text_original.translate(str.maketrans('','',string.punctuation))\n","    return(text_no_punctuation)\n","text_no_punctuation = remove_punctuation(text_original)\n","\n","def remove_single_character(text):\n","    text_len_more_than1 = \"\"\n","    for word in text.split():\n","        if len(word) > 1:\n","            text_len_more_than1 += \" \" + word\n","    return(text_len_more_than1)\n","text_len_more_than1 = remove_single_character(text_no_punctuation)\n","\n","def remove_numeric(text,printTF=False):\n","    text_no_numeric = \"\"\n","    for word in text.split():\n","        isalpha = word.isalpha()\n","        if printTF:\n","            print(\" {:10} : {:}\".format(word, isalpha))\n","        if isalpha:\n","            text_no_numeric += \" \" + word\n","    return(text_no_numeric)\n","text_no_numeric = remove_numeric(text_len_more_than1,printTF=True)\n","print(text_no_numeric)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def text_clean(text_original):\n","    text = remove_punctuation(text_original)\n","    text = remove_single_character(text)\n","    text = remove_numeric(text)\n","    return(text)\n","\n","for i, caption in enumerate(df_txt.caption.values):\n","    newcaption = text_clean(caption)\n","    df_txt[\"caption\"].iloc[i] = newcaption"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfword = df_word(df_txt)\n","plthist(dfword.iloc[:topn,:], title=\"The top 50 frequently appearing words\")\n","plthist(dfword.iloc[-topn:,:], title=\"The least 50 most frequently appearing words\")"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from copy import copy\n","def add_start_end_seq_token(captions):\n","    caps = []\n","    for txt in captions:\n","        txt = 'startseq ' + txt + ' endseq'\n","        caps.append(txt)\n","    return(caps)\n","df_txt0 = copy(df_txt)\n","df_txt0[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\n","df_txt0.head(5)\n","del df_txt"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelvgg = tf.keras.applications.VGG16(include_top=True, weights=None)\n","## Load the locally saved weights\n","modelvgg.load_weights(\"../input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\n","modelvgg.summary()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelvgg.layers.pop()\n","modelvgg = tf.keras.Model(inputs=modelvgg.inputs, outputs=modelvgg.layers[-2].output)\n","## show the deep learning model\n","modelvgg.summary()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.image import load_img, img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from collections import OrderedDict\n","\n","images = OrderedDict()\n","npix = 224\n","target_size = (npix,npix,3)\n","data = np.zeros((len(jpgs),npix,npix,3))\n","for i, name in enumerate(jpgs):\n","    # Load on image from file\n","    filename = dir_Flickr_jpg + '/' + name\n","    image = load_img(filename, target_size=target_size)\n","    # convert the image pixels to a numpy array\n","    image = img_to_array(image)\n","    nimage = preprocess_input(image)\n","\n","    y_pred = modelvgg.predict(nimage.reshape( (1,) + nimage.shape[:3]))\n","    images[name] = y_pred.flatten()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images['19212715_20476497a3.jpg'].shape"],"metadata":{"trusted":true},"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'images' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-6-e2c94220472e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'19212715_20476497a3.jpg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"]}]},{"cell_type":"code","source":["len(images)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dimages, keepindex = [],[]\n","df_txt0 = df_txt0.loc[df_txt0[\"index\"].values == \"0\",: ]\n","for i, fnm in enumerate(df_txt0.filename):\n","    if fnm in images.keys():\n","        dimages.append(images[fnm])\n","        keepindex.append(i)\n","\n","fnames = df_txt0[\"filename\"].iloc[keepindex].values\n","dcaptions = df_txt0[\"caption\"].iloc[keepindex].values\n","dimages = np.array(dimages)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4096 features for all 8091 images**"],"metadata":{}},{"cell_type":"code","source":["dimages.shape"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**list of captions for each image**"],"metadata":{}},{"cell_type":"code","source":["dcaptions[:5]"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenizer\n","\n","the maximum number of words in dictionary"],"metadata":{}},{"cell_type":"code","source":["\n","from keras.preprocessing.text import Tokenizer\n","\n","nb_words = 8000\n","tokenizer = Tokenizer(nb_words=nb_words)\n","tokenizer.fit_on_texts(dcaptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print(\"Vocabulary size : {}\".format(vocab_size))\n","dtexts = tokenizer.texts_to_sequences(dcaptions)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dtexts[:5]"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**split the dataset in ratio 6:2:2 (train:valid:test)**"],"metadata":{}},{"cell_type":"code","source":["prop_test, prop_val = 0.2, 0.2\n","N = len(dtexts)\n","Ntest, Nval = int(N*prop_test), int(N*prop_val)\n","\n","def split_test_val_train(dtexts, Ntest,Nval):\n","    return(dtexts[:Ntest],dtexts[Ntest:Ntest+Nval],dtexts[Ntest+Nval:])\n","\n","dt_test, dt_val, dt_train = split_test_val_train(dtexts,Ntest,Nval)\n","di_test, di_val, di_train = split_test_val_train(dimages,Ntest,Nval)\n","fnm_test, fnm_val, fnm_train = split_test_val_train(fnames,Ntest,Nval)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["maxlen = np.max([len(text) for text in dtexts])"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","\n","def preprocessing(dtexts, dimages):\n","    N = len(dtexts)\n","    print(\"# captions/images = {}\".format(N))\n","\n","    assert(N==len(dimages))\n","    Xtext, Ximage, ytext = [],[],[]\n","    for text, image in zip(dtexts,dimages):\n","        for i in range(1,len(text)):\n","            in_text, out_text = text[:i],text[i]\n","            in_text = pad_sequences([in_text],maxlen=maxlen).flatten()\n","            out_text = to_categorical(out_text,num_classes = vocab_size)\n","\n","            Xtext.append(in_text)\n","            Ximage.append(image)\n","            ytext.append(out_text)\n","\n","    Xtext = np.array(Xtext)\n","    Ximage = np.array(Ximage)\n","    ytext = np.array(ytext)\n","    print(\" {} {} {} \".format(Xtext.shape,Ximage.shape,ytext.shape))\n","    return(Xtext,Ximage,ytext)\n","\n","Xtext_train, Ximage_train, ytext_train = preprocessing(dt_train,di_train)\n","Xtext_val, Ximage_val, ytext_val = preprocessing(dt_val,di_val)\n","# pre-processing is not necessary for testing data\n","#Xtext_test, Ximage_test, ytext_test = preprocessing(dt_test,di_test)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Xtext_train[:14]"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Ximage_train.shape"],"metadata":{"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"(49631, 4096)"},"metadata":{}}]},{"cell_type":"code","source":["ytext_train.shape"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["** Model**"],"metadata":{}},{"cell_type":"code","source":["\n","Ximage_train.shape[1]"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import layers\n","print(vocab_size)\n","\n","dim_embedding = 64\n","\n","input_image = layers.Input(shape=(Ximage_train.shape[1],))\n","fimage = layers.Dense(256,activation='relu',name='ImageFeature')(input_image)\n","\n","## sequence model\n","input_txt = layers.Input(shape=(maxlen,))\n","\n","## the embedding layer in keras can be used when we want to create the embeddings toembed higher dimensional data into lower \n","ftxt = layers.Embedding(vocab_size,dim_embedding,mask_zero=True)(input_txt)\n","ftxt = layers.LSTM(256,name=\"CaptionFeature\")(ftxt)\n","\n","## combined model far decoder\n","decoder = layers.add([ftxt,fimage])\n","decoder = layers.Dense(256,activation='relu')(decoder)\n","output = layers.Dense(vocab_size,activation='softmax')(decoder)\n","model = tf.keras.Model(inputs=[input_image,input_txt],outputs=output)\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","print(model.summary())"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["** here we are giving one image and a start-word and we want to predict the next word.\n"," the next word is in our target which is 75**"],"metadata":{}},{"cell_type":"code","source":["print(Ximage_train[0])\n","print(Xtext_train[0])\n","print(ytext_train[0][75])"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time() \n","hist = model.fit(x= [Ximage_train,Xtext_train],y= ytext_train, batch_size= 64, epochs=5, verbose=2,validation_split=0,validation_data=([Ximage_val, Xtext_val],ytext_val))\n","\n","end = time.time()\n","print(\"Time Took {:3.2f}MIN\".format((end - start)/60))"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(Ximage_train.shape,Xtext_train.shape,ytext_train.shape)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for label in [\"loss\",\"val_loss\"]:\n","    plt.plot(hist.history[label],label=label)\n","plt.legend()\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","plt.show()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n","def predict_caption(image):\n","    '''\n","    image.shape = (1,4462)\n","    '''\n","\n","    in_text = 'startsep'\n","    for iword in range(maxlen):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence],maxlen)\n","        yhat = model.predict([image,sequence],verbose=0)\n","        yhat = np.argmax(yhat)\n","        newword = index_word[yhat]\n","        in_text += \" \" + newword\n","        if newword == \"endseq\":\n","            break\n","    return(in_text)\n","\n","npic = 5\n","npix = 224\n","target_size = (npix,npix,3)\n","\n","count = 1\n","fig = plt.figure(figsize=(10,20))\n","for jpgfnm, image_feature in zip(fnm_test[:npic],di_test[:npic]):\n","    ## images\n","    filename = dir_Flickr_jpg + '/' + jpgfnm\n","    image_load = load_img(filename, target_size=target_size)\n","    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n","    ax.imshow(image_load)\n","    count += 1\n","\n","    ## caption\n","    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n","    ax = fig.add_subplot(npic,2,count)\n","    plt.axis('off')\n","    ax.plot()\n","    ax.set_xlim(0,1)\n","    ax.set_ylim(0,1)\n","    ax.text(0,0.5,caption,fontsize=28)\n","    count += 1\n","\n","plt.show()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images['1000268201_693b08cb0e.jpg'].shape"],"metadata":{"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"(4096,)"},"metadata":{}}]},{"cell_type":"markdown","source":["**BLEU SCORE**"],"metadata":{}},{"cell_type":"code","source":["start = time.time()\n","from nltk.translate.bleu_score import sentence_bleu\n","index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n","\n","nkeep = 5\n","pred_good, pred_bad, bleus = [], [], []\n","couunt = 0\n","for jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n","    count += 1\n","    if count % 200 == 0:\n","        print(\" {:4.2f} % is done....\".format(100*count/float(len(fnm_test))))\n","    caption_true = [ index_word[i] for i in tokenized_text ]\n","    caption_true = caption_true[1:-1] ## remove startreg, and endreg\n","    ## captions\n","    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n","    caption = caption.split()\n","    caption = caption[1:-1] ## remove startreg and endreg\n","    \n","    bleu = sentence_bleu([caption_true],caption)\n","    bleus.append(bleu)\n","    if bleu > 0.6 and len(pred_good) < nkeep:\n","        pred_good.append((bleu,jpgfnm,caption_true,caption))\n","    elif bleu < 0.3 and len(pred_bad) < nkeep:\n","        pred_bad.append((bleu,jpgfnm,caption_true,caption))\n","\n","end = time.time()\n","print((start - end)/60)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_good"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_bad[:2]"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Mean BLEU {:4.3f}\".format(np.mean(bleus)))"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_images(pred_bad):\n","    def create_str(caption_true):\n","        strue = \"\"\n","        for s in caption_true:\n","            strue += \" \" + s\n","        return(strue)\n","    npix = 224\n","    taret_size = (npix,npix,3)\n","    count = 1\n","    fig = plt.figure(figsize=(10,20))\n","    npic = len(pred_bad)\n","    for pd in pred_bad:\n","        bleu,jpgfnm,caption_true,caption = pd\n","        ## image\n","        filename = dir_Flickr_jpg + '/' + jpgfnm\n","        image_load = load_img(filename, target_size=target_size)\n","        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n","        ax.imshow(image_load)\n","        count += 1\n","        \n","        caption_true = create_str(caption_true)\n","        caption = create_str(caption)\n","        \n","        ax = fig.add_subplot(npic,2,count)\n","        plt.axis('off')\n","        ax.plot()\n","        ax.set_xlim(0,1)\n","        ax.set_ylim(0,1)\n","        ax.text(0,0.7,\"true:\" + caption_true,fontsize=20)\n","        ax.text(0,0.4,\"pred:\" + caption, fontsize=20)\n","        ax.text(0,0.1,\"BLEU: {}\".format(bleu),fontsize=20)\n","        count += 1\n","    plt.show()\n","    \n","print(\"Bad Caption\")\n","plot_images(pred_bad)\n","print(\"Good Caption\")\n","plot_images(pred_good)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**GUI**"],"metadata":{}},{"cell_type":"code","source":["#import libraries\n","\n","import tkinter as tk\n","from PIL import Image, ImageTk\n","from tkinter import filedialog"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","os.chdir('C:/Users/Omshree/Desktop/NLP')\n","os.getcwd()"],"metadata":{"trusted":true},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'C:\\\\Users\\\\Omshree\\\\Desktop\\\\NLP'"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["def upload_img():\n","    global img, image_date\n","    for img_display in frame.winfo_children():\n","        img_display.destroy()\n","    \n","    image_data = filedialog.askopenfilename(initaldir=\"/\", title=\"Choose an image\", filetypes=((\"all files\",\"*.*\"),(\"png files\",\"*.png\")))\n","    basewidth = 300\n","    img = Image.open(image_data)\n","    wpercent = (basewidth / float(img.size[0]))\n","    hsize = int((float(img.size[1]) * float(wpercent)))\n","    img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n","    img = ImageTk.PhotoImage(img)\n","    file_name = image_data.split('/')\n","    panel = tk.Label(frame, text=str(file_name[len(file_name)-1]).upper()).pack()\n","    panel_image = tk.Label(frame, image=img).pack()\n","    \n","def caption():\n","    original = Image.open(image_data)\n","    original = original.resize((224,224), Image.ANTIALIAS)\n","    numpy_image = img_to_array(original)\n","    nimage = preprocess_input(numpy_image)\n","    \n","    feature = modelvgg.predict(nimage.reshape((1,) + nimage.shape[:3]))\n","    caption = predict_caption(feature)\n","    table = tk.Label(frame, text=\"Caption: \" + caption[9:-7],font=(\"Helvetica\",12)).pack()"],"metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["root = tk.Tk()\n","root.title('IMAGE CAPTION NLP')\n","root.iconbitmap('class.ico')\n","root.resizable(False, False)\n","tit = tk.Label(root, text=\"IMAGE CAPTION GENERATOR\", padx=25, pady=6, font=(\"\",12)).pack()\n","canvas = tk.Canvas(root, height=500, width=600, bg='#D1EDf2')\n","canvas.pack()\n","frame = tk.Frame(root, bg='white')\n","frame.place(relwidth=0.8, relheight=0.8, relx=0.1, rely=0.1)\n","chose_image = tk.Button(root, text='Choose Image',padx=35,pady=10,fg=\"black\",bg=\"pink\",command=upload_img, activebackground=\"#add8e6\")\n","chose_image.pack(side=tk.LEFT)\n","\n","caption_image = tk.Button(root, text='Classify Image',padx=35, pady=10,fg=\"black\", bg=\"pink\", command=caption, activebackground=\"#add8e6\")\n","caption_image.pack(side=tk.Right)\n","root.mainloop()"],"metadata":{"trusted":true},"execution_count":88,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-88-27258c664746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IMAGE CAPTION NLP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miconbitmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class.ico'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresizable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"IMAGE CAPTION GENERATOR\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpady\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"],"ename":"TclError","evalue":"no display name and no $DISPLAY environment variable","output_type":"error"}]}]}